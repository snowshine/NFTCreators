{
 "cells": [
  {
   "cell_type": "raw",
   "id": "575ad400",
   "metadata": {},
   "source": [
    "Helper functions to filter full collectons to smaller subset based on rarity score total or total price... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e94871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d61bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to metdata zip files, uncomment the collection to be processed\n",
    "\n",
    "#metadata_zip_filename = \"/data/cryptopunks.zip\"\n",
    "metadata_zip_filename = \"/data/boredapeyachtclub_metadata_archive.zip\"\n",
    "#metadata_zip_filename = \"/data/hapeprime_metadata_archive.zip\"\n",
    "#metadata_zip_filename = \"/data/meebits_metadata_archive.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c48132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide a list of token names ordered by highest rarity score\n",
    "#path and filename to zip that has the metadata!\n",
    "#percent of zipcollection to pull 0 to 1.0, default all\n",
    "def filterCollectionByRareTraits(metadata_zip_fileName, percent_collection = 1):\n",
    "    \n",
    "    # open zipped dataset \n",
    "    collection_list = list()\n",
    "    collection_df = None\n",
    "    traits_df = None\n",
    "    full_traits_list = list()\n",
    "    collection_count = 0\n",
    "    with zipfile.ZipFile(metadata_zip_filename) as z:\n",
    "\n",
    "        listOfiles = z.namelist()\n",
    "        for fileName in listOfiles:\n",
    "            #just the json\n",
    "            if \".json\" in fileName:\n",
    "                collection_count = collection_count + 1\n",
    "                with z.open(fileName) as f:\n",
    "\n",
    "                    #read nft metadata into a df\n",
    "                    df = pd.read_json(f, orient='index')\n",
    "                    df = df.reset_index()            \n",
    "\n",
    "                    #creates traits dict list\n",
    "                    traits_series = df[df['index'] == 'traits'][0]\n",
    "                    traits_list = traits_series.to_list()[0]\n",
    "                    #add id field to each trait_dict in list\n",
    "                    id_val = df[df['index'] == 'token_id'][0].to_list()[0]\n",
    "                    for traits_dict in traits_list:\n",
    "                        traits_dict['token_id'] = id_val\n",
    "                    full_traits_list.extend(traits_list)\n",
    "\n",
    "    #create dataframe with token_id, trait and trait count\n",
    "    traits_df = pd.DataFrame(full_traits_list)\n",
    "    traits_df = traits_df.astype({\"token_id\": int, \"trait_count\": int})\n",
    "\n",
    "    #calculate trait rarity score based on rarity.tools approach\n",
    "    traits_score_df = traits_df.groupby(['trait_type','value']).size().reset_index()\n",
    "    traits_score_df['trait_type_value'] = traits_score_df['trait_type'] + traits_score_df['value'] \n",
    "    traits_score_df.columns = ['trait_type', 'value', 'total count', 'trait_type_value',]\n",
    "    traits_score_df = traits_score_df.sort_values(by=['total count'], ascending=False)\n",
    "    traits_score_df = traits_score_df.astype({\"total count\": int})\n",
    "    traits_score_df['rarity_score'] = 1/(traits_score_df['total count']/collection_count) \n",
    "    traits_score_df['rarity_score_log'] = np.log10(traits_score_df['rarity_score'])\n",
    "    traits_score_df = traits_score_df[['trait_type_value', 'rarity_score', 'rarity_score_log']] \n",
    "\n",
    "    #sum trait score based on rarity.tools calculation\n",
    "    traits_group_df = traits_df.groupby(['token_id','trait_type','value']).size().reset_index()\n",
    "    traits_group_df.rename({'0': 'count'}, axis=1, inplace=True)\n",
    "    traits_group_df.columns = ['token_id','trait_type', 'trait_value', 'total_count']\n",
    "    #combine trait type and value as the key\n",
    "    traits_group_df['trait_type_value'] = traits_group_df['trait_type'] + traits_group_df['trait_value']\n",
    "\n",
    "    #merge with traits_score_df\n",
    "    token_traits_rarity_total_df = pd.merge(traits_group_df, traits_score_df, on='trait_type_value')\n",
    "    #group tokens on rarity_score total\n",
    "    token_traits_rarity_total_df = token_traits_rarity_total_df.groupby(['token_id']).agg({'rarity_score': ['sum'], })\n",
    "    token_traits_rarity_total_df.columns = ['rarity_total']\n",
    "    token_traits_rarity_total_df = token_traits_rarity_total_df.reset_index()\n",
    "    #sort on the highest rarity score\n",
    "    token_traits_rarity_total_df = token_traits_rarity_total_df.sort_values(by=['rarity_total'], ascending=False)\n",
    "    \n",
    "    #pull the percentage of collection requested\n",
    "    col_size = int(percent_collection * collection_count)\n",
    "    return token_traits_rarity_total_df.head(col_size)['token_id'].to_list()        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9861d83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grap token list needed by create_filtered_collection_zip function\n",
    "tokens_list = filterCollectionByRareTraits(metadata_zip_filename, .01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed0d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide a list of token names ordered by highest total price \n",
    "#path and filename to zip that has the metadata!\n",
    "#percent of zipcollection to pull 0 to 1.0, default all\n",
    "def filterCollectionByHighestTotalPrice(metadata_zip_fileName, percent_collection = 1):\n",
    "    \n",
    "    full_sales_list = list()\n",
    "    sales_df = None\n",
    "    collection_count = 0\n",
    "    # open zipped dataset\n",
    "    with zipfile.ZipFile(metadata_zip_filename) as z:\n",
    "\n",
    "        listOfiles = z.namelist()\n",
    "        for fileName in listOfiles:\n",
    "            #just the json\n",
    "            if \".json\" in fileName:\n",
    "                collection_count = collection_count + 1\n",
    "                with z.open(fileName) as f:\n",
    "\n",
    "                    #read nft metadata into a df\n",
    "                    df = pd.read_json(f, orient='index')\n",
    "                    df = df.reset_index()            \n",
    "\n",
    "                    #create costs list\n",
    "                    field_dic = df[df['index'] == 'last_sale'][0].to_list()[0]\n",
    "\n",
    "                    if field_dic is not None:\n",
    "\n",
    "                        sales_dic = dict()\n",
    "                        sales_dic['token_id'] = field_dic['asset']['token_id']\n",
    "                        sales_dic['total_price'] = int(field_dic['total_price'])\n",
    "                        if field_dic['quantity'] is None:\n",
    "                                sales_dic['quantity'] = 1\n",
    "                        else:\n",
    "                            sales_dic['quantity'] = int(field_dic['quantity'])\n",
    "                        sales_dic['event_timestamp'] = field_dic['event_timestamp']\n",
    "                        sales_dic['decimals'] = int(field_dic['payment_token']['decimals'])\n",
    "                        sales_dic['eth_price'] = float(field_dic['payment_token']['eth_price'])\n",
    "                        sales_dic['usd_price'] = float(field_dic['payment_token']['usd_price'])\n",
    "\n",
    "                        # Converting sales price from WEI to ETH then USD in log\n",
    "                        sales_dic['total_price_usd'] = sales_dic['total_price']/(10**sales_dic['decimals'])  * sales_dic['usd_price']/sales_dic['quantity']\n",
    "                        if sales_dic['total_price_usd'] > 0:\n",
    "                            sales_dic['total_price_usd_log'] = np.log10(sales_dic['total_price_usd'])\n",
    "                        else:\n",
    "                            sales_dic['total_price_usd_log'] = 0\n",
    "                        full_sales_list.append(sales_dic)\n",
    "\n",
    "    sales_df = pd.DataFrame(full_sales_list)\n",
    "    sales_df = sales_df.astype({\"token_id\": int, \"total_price\": float, \"eth_price\": float, \"usd_price\": float, \"total_price_usd\":float, \"total_price_usd_log\":float})\n",
    "    \n",
    "    #sort on the highest total price usd log\n",
    "    sales_df = sales_df.sort_values(by=['total_price_usd_log'], ascending=False)\n",
    "    #print(sales_df.head(5))\n",
    "    \n",
    "    #pull the percentage of collection requested\n",
    "    col_size = int(percent_collection * collection_count)\n",
    "    return sales_df.head(col_size)['token_id'].to_list()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa853a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test get tokens py price\n",
    "tokens_list = filterCollectionByHighestTotalPrice(metadata_zip_filename, .01)\n",
    "#print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379830af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new image_zip file that is a subset based on tokens_list\n",
    "#src_zip that has the image files!\n",
    "#dst_zip is the new zip that contains the subset of png files based on tokens list\n",
    "#tokens_list generated from filterCollectionBy... functions\n",
    "\n",
    "def create_filtered_collection_zip(src_zip, dst_zip, tokens_list):\n",
    "    #conv token list to str for matching\n",
    "    tokens_list = list(map(str, tokens_list))\n",
    "    with ZipFile(src_zip, \"r\", compression=ZIP_DEFLATED) as src_zip_archive:\n",
    "        with ZipFile(dst_zip, \"w\", compression=ZIP_DEFLATED) as dst_zip_archive:\n",
    "            for zitem in src_zip_archive.namelist():\n",
    "                #if zitem in tokens_list and zitem:\n",
    "                filename = os.path.basename(zitem)\n",
    "                # skip directories and json files\n",
    "                if not filename or \".png\" not in filename:\n",
    "                    continue\n",
    "                else:\n",
    "                    #strip off .png extension to find file and check in tokens_list\n",
    "                    if filename[:-4] in tokens_list:\n",
    "                        #print(filename)\n",
    "                        if sys.version_info >= (3, 6):\n",
    "                            with src_zip_archive.open(zitem) as from_item:\n",
    "                                with dst_zip_archive.open(zitem, \"w\") as to_item:\n",
    "                                    shutil.copyfileobj(from_item, to_item)\n",
    "                        else:\n",
    "                            # warning, may blow up memory\n",
    "                            dst_zip_archive.writestr(zitem, \n",
    "                            src_zip_archive.read(zitem))\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "743e251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new metadata_zip file that is a subset based on tokens_list\n",
    "#src_zip that has the metadata files!\n",
    "#dst_zip is the new zip that contains the subset of metadata files based on tokens list\n",
    "#tokens_list generated from filterCollectionBy... functions\n",
    "\n",
    "def create_filtered_collection_metadata_zip(src_zip, dst_zip, tokens_list):\n",
    "    #conv token list to str for matching\n",
    "    tokens_list = list(map(str, tokens_list))\n",
    "    with ZipFile(src_zip, \"r\", compression=ZIP_DEFLATED) as src_zip_archive:\n",
    "        with ZipFile(dst_zip, \"w\", compression=ZIP_DEFLATED) as dst_zip_archive:\n",
    "            for zitem in src_zip_archive.namelist():\n",
    "                #if zitem in tokens_list and zitem:\n",
    "                filename = os.path.basename(zitem)\n",
    "                # skip directories and json files\n",
    "                if not filename or \".png\" in filename:\n",
    "                    continue\n",
    "                else:\n",
    "                    #strip off .png extension to find file and check in tokens_list\n",
    "                    if filename[:-5] in tokens_list:\n",
    "                        #print(filename)\n",
    "                        if sys.version_info >= (3, 6):\n",
    "                            with src_zip_archive.open(zitem) as from_item:\n",
    "                                with dst_zip_archive.open(zitem, \"w\") as to_item:\n",
    "                                    shutil.copyfileobj(from_item, to_item)\n",
    "                        else:\n",
    "                            # warning, may blow up memory\n",
    "                            dst_zip_archive.writestr(zitem, \n",
    "                            src_zip_archive.read(zitem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34dd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "#update for location of image zip and metadata archive to be processed\n",
    "\n",
    "#images_in_zip_filename = \"/data/cryptopunks.zip\"\n",
    "images_in_zip_filename = \"/data/boredapeyachtclub_archive.zip\"\n",
    "metadata_zip_filename = \"/data/boredapeyachtclub_metadata_archive.zip\"\n",
    "\n",
    "#name of zip to create\n",
    "#zip_out_filename = \"/data/cryptopunks_filtered.zip\"\n",
    "zip_out_filename = \"/data/boredapeyachtclub_filtered.zip\"\n",
    "metadata_zip_out_filename = \"/data/boredapeyachtclub_metadata_filtered.zip\"\n",
    "\n",
    "\n",
    "create_filtered_collection_zip(images_in_zip_filename, zip_out_filename, tokens_list)\n",
    "create_filtered_collection_metadata_zip(metadata_zip_filename, metadata_zip_out_filename, tokens_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87b6a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve the valuable list of apes being trained in the model, pkl file contains dataframe with tokens of interest to\n",
    "#create collection, output used to train the filtered most valuable Bored Ape model, which is served by the web app \n",
    "df = pd.read_pickle('subset_metadata.pkl')\n",
    "\n",
    "valuable_apes_tokens_list = df['token_id'].to_list()\n",
    "#print(valuable_apes_tokens_list)\n",
    "\n",
    "metadata_zip_filename = \"/data/boredapeyachtclub_metadata_archive.zip\"\n",
    "zip_out_filename = \"/data/boredapeyachtclub_valuable_metadata_archive.zip\"\n",
    "\n",
    "create_filtered_collection_metadata_zip(metadata_zip_filename, zip_out_filename, valuable_apes_tokens_list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
